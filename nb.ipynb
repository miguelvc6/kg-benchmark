{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0168ba74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import mwclient\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "SITE = mwclient.Site('www.wikidata.org')\n",
    "# Example: P569 (Date of Birth) is a classic source of \"Logical\" (Type A) constraints\n",
    "# You can add more properties here: P570 (Date of Death), P21 (Sex or Gender)\n",
    "TARGET_PROPERTIES = ['P569'] \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51f21ff",
   "metadata": {},
   "source": [
    "## Index candidates\n",
    "\n",
    "Wikidata generates static pages listing current violations (e.g., Wikidata:Database reports/Constraint violations/Summary).\n",
    "\n",
    "Strategy: Parse the history of these report pages. If an Entity $X$ appears in the \"Violation Report\" on Date $T_1$ and disappears on Date $T_2$, a repair event occurred between $T_1$ and $T_2$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87430867",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_report_page_title(property_id):\n",
    "    \"\"\"\n",
    "    Returns the standard location where Wikidata bots publish violation lists.\n",
    "    \"\"\"\n",
    "    return f\"Wikidata:Database reports/Constraint violations/{property_id}\"\n",
    "\n",
    "\n",
    "def extract_qids(text):\n",
    "    \"\"\"\n",
    "    Extracts all Q-IDs (e.g., Q12345) from the report page text.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return set()\n",
    "    # Regex to find [[Q123]] patterns which is standard in report pages\n",
    "    return set(re.findall(r\"\\[\\[(Q\\d+)\\]\\]\", text))\n",
    "\n",
    "\n",
    "def mine_repairs(property_id, max_items=50):\n",
    "    print(f\"[*] Mining history for {property_id}...\")\n",
    "    page = SITE.pages[get_report_page_title(property_id)]\n",
    "\n",
    "    if not page.exists:\n",
    "        print(f\"[!] Report page for {property_id} not found.\")\n",
    "        return []\n",
    "\n",
    "    candidates = []\n",
    "\n",
    "    # We iterate backwards through history\n",
    "    # We need to compare Rev_N (Older) vs Rev_N+1 (Newer)\n",
    "    # If Q-ID is in Old but NOT in New -> It was fixed (or deleted)\n",
    "\n",
    "    revisions = list(page.revisions(max_items=max_items, prop=\"content|timestamp|ids\"))\n",
    "    print(f\"    Found {len(revisions)} revisions to analyze.\")\n",
    "\n",
    "    # Iterate pairwise\n",
    "    for i in range(len(revisions) - 1):\n",
    "        newer_rev = revisions[i]\n",
    "        older_rev = revisions[i + 1]\n",
    "\n",
    "        # Extract QIDs\n",
    "        qids_old = extract_qids(older_rev.get(\"*\", \"\"))\n",
    "        qids_new = extract_qids(newer_rev.get(\"*\", \"\"))\n",
    "\n",
    "        fixed_qids = qids_old - qids_new\n",
    "\n",
    "        if fixed_qids:\n",
    "            timestamp = datetime(*newer_rev[\"timestamp\"][:6]).isoformat()\n",
    "            print(\n",
    "                f\"    [{timestamp}] Found {len(fixed_qids)} fixes (Rev {older_rev['revid']} -> {newer_rev['revid']})\"\n",
    "            )\n",
    "\n",
    "            for qid in fixed_qids:\n",
    "                candidates.append(\n",
    "                    {\n",
    "                        \"qid\": qid,\n",
    "                        \"property_id\": property_id,\n",
    "                        \"fix_date\": timestamp,\n",
    "                        \"report_revision_old\": older_rev[\"revid\"],\n",
    "                        \"report_revision_new\": newer_rev[\"revid\"],\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    return candidates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa08e76",
   "metadata": {},
   "source": [
    "Store in a json file with schema\n",
    "\n",
    "```\n",
    "[\n",
    "  {\n",
    "    \"qid\": \"Q137258824\",\n",
    "    \"property_id\": \"P569\",\n",
    "    \"fix_date\": \"2025-12-10T09:36:47\",\n",
    "    \"report_revision_old\": 2439960515,\n",
    "    \"report_revision_new\": 2440403690\n",
    "  },\n",
    "  ...\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8593345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[+] repair_candidates.json Exists. Loaded from disk. Found 304 candidates.\n"
     ]
    }
   ],
   "source": [
    "filename = \"repair_candidates.json\"\n",
    "all_candidates = []\n",
    "\n",
    "if os.path.exists(filename):\n",
    "    with open(filename) as f:\n",
    "        all_candidates = json.load(f)\n",
    "    if isinstance(all_candidates, list) and all_candidates:\n",
    "        print(f\"\\n[+] {filename} Exists. Loaded from disk. Found {len(all_candidates)} candidates.\")\n",
    "    else:\n",
    "        all_candidates = []\n",
    "\n",
    "if not all_candidates:\n",
    "    print(f\"\\n[!] {filename} does not exist. Starting fresh...\")\n",
    "    for prop in TARGET_PROPERTIES:\n",
    "        all_candidates.extend(mine_repairs(prop, max_items=20))  # Start small for testing\n",
    "\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(all_candidates, f, indent=2)\n",
    "\n",
    "    print(f\"\\n[+] Done. Found {len(all_candidates)} candidates. Saved to {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cf8e0e",
   "metadata": {},
   "source": [
    "## Fetching the Repair\n",
    "\n",
    "Once we have the Candidate Entity ID and the Time Window ($T_1, T_2$), we use the Wikibase REST API.\n",
    "\n",
    "Endpoint: GET /w/rest.php/v1/page/{page_title}/history\n",
    "\n",
    "We are looking for the specific revision $R$ where the constraint status changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7167e954",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from pathlib import Path\n",
    "\n",
    "import ijson\n",
    "import requests\n",
    "\n",
    "HEADERS = {\"User-Agent\": \"WikidataRepairEval/1.0 (PhD Research; mailto:miguel.vazquez@wu.ac.at)\"}\n",
    "API_ENDPOINT = \"https://www.wikidata.org/w/api.php\"\n",
    "REST_HISTORY_URL = \"https://www.wikidata.org/w/rest.php/v1/page/{qid}/history\"\n",
    "ENTITY_DATA_URL = \"https://www.wikidata.org/wiki/Special:EntityData/{qid}.json\"\n",
    "\n",
    "STRICT_PERSISTENCE = True\n",
    "API_TIMEOUT = 30\n",
    "REVISION_LOOKBACK_DAYS = 30\n",
    "MAX_HISTORY_PAGES = 8\n",
    "MAX_PROPERTY_VALUES = 12\n",
    "MAX_NEIGHBOR_EDGES = 50\n",
    "LATEST_DUMP_PATH = Path(\"latest-all.json.gz\")\n",
    "WORLD_STATE_FILE = Path(\"world_state.json\")\n",
    "\n",
    "RUN_ID = datetime.utcnow().strftime(\"%Y%m%dT%H%M%S\")\n",
    "LOG_DIR = Path(\"logs\")\n",
    "LOG_DIR.mkdir(exist_ok=True)\n",
    "STATS_FILE = LOG_DIR / f\"fetcher_stats_{RUN_ID}.jsonl\"\n",
    "SUMMARY_FILE = LOG_DIR / f\"run_summary_{RUN_ID}.json\"\n",
    "\n",
    "\n",
    "class StatsLogger:\n",
    "    def __init__(self, stats_path):\n",
    "        self.stats_path = stats_path\n",
    "        self.run_id = RUN_ID\n",
    "\n",
    "    def log(self, record):\n",
    "        enriched = {\"run_id\": self.run_id}\n",
    "        enriched.update(record)\n",
    "        with open(self.stats_path, \"a\", encoding=\"utf-8\") as fh:\n",
    "            fh.write(json.dumps(enriched, ensure_ascii=True))\n",
    "            fh.write(\"\\n\")\n",
    "\n",
    "\n",
    "def pick_label(entity, lang=\"en\"):\n",
    "    if not entity:\n",
    "        return None\n",
    "    labels = entity.get(\"labels\", {})\n",
    "    if lang in labels:\n",
    "        return labels[lang].get(\"value\")\n",
    "    if labels:\n",
    "        first = next(iter(labels.values()))\n",
    "        return first.get(\"value\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def pick_description(entity, lang=\"en\"):\n",
    "    if not entity:\n",
    "        return None\n",
    "    descriptions = entity.get(\"descriptions\", {})\n",
    "    if lang in descriptions:\n",
    "        return descriptions[lang].get(\"value\")\n",
    "    if descriptions:\n",
    "        first = next(iter(descriptions.values()))\n",
    "        return first.get(\"value\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def chunked(iterable, size):\n",
    "    batch = []\n",
    "    for item in iterable:\n",
    "        batch.append(item)\n",
    "        if len(batch) == size:\n",
    "            yield batch\n",
    "            batch = []\n",
    "    if batch:\n",
    "        yield batch\n",
    "\n",
    "\n",
    "class WorldStateBuilder:\n",
    "    def __init__(self, dump_path):\n",
    "        self.dump_path = Path(dump_path)\n",
    "        self.has_dump = self.dump_path.exists()\n",
    "\n",
    "    def build(self, entries):\n",
    "        if not entries:\n",
    "            return {}\n",
    "        if not self.has_dump:\n",
    "            print(f\"[!] Context Builder skipped: dump not found at {self.dump_path}\")\n",
    "            return {}\n",
    "\n",
    "        focus_ids = {entry[\"qid\"] for entry in entries}\n",
    "        property_ids = {entry[\"property\"] for entry in entries}\n",
    "        combined_targets = focus_ids | property_ids\n",
    "\n",
    "        print(f\"[*] Context Builder: streaming dump for {len(combined_targets)} focus/constraint entities...\")\n",
    "        combined_entities = self._load_entities_from_dump(combined_targets)\n",
    "        focus_entities = {eid: combined_entities[eid] for eid in focus_ids if eid in combined_entities}\n",
    "        property_entities = {pid: combined_entities[pid] for pid in property_ids if pid in combined_entities}\n",
    "\n",
    "        missing_focus = focus_ids - set(focus_entities)\n",
    "        if missing_focus:\n",
    "            print(f\"    [!] Missing {len(missing_focus)} focus entities in dump.\")\n",
    "\n",
    "        neighbor_ids = self._collect_neighbor_targets(focus_entities.values())\n",
    "        print(f\"[*] Context Builder: streaming dump for {len(neighbor_ids)} neighbor entities...\")\n",
    "        neighbor_entities = self._load_entities_from_dump(neighbor_ids)\n",
    "        missing_neighbors = neighbor_ids - set(neighbor_entities)\n",
    "        if missing_neighbors:\n",
    "            print(f\"    [!] {len(missing_neighbors)} neighbors not found in dump. Falling back to Action API for labels.\")\n",
    "            neighbor_entities.update(self._fetch_labels_via_api(missing_neighbors))\n",
    "\n",
    "        world_states = {}\n",
    "        for entry in entries:\n",
    "            focus_entity = focus_entities.get(entry[\"qid\"])\n",
    "            if not focus_entity:\n",
    "                continue\n",
    "            property_entity = property_entities.get(entry[\"property\"])\n",
    "            context = self._assemble_world_state(\n",
    "                focus_entity,\n",
    "                neighbor_entities,\n",
    "                entry[\"property\"],\n",
    "                property_entity,\n",
    "            )\n",
    "            world_states[entry[\"id\"]] = context\n",
    "        return world_states\n",
    "\n",
    "    def _load_entities_from_dump(self, target_ids):\n",
    "        found = {}\n",
    "        if not target_ids or not self.has_dump:\n",
    "            return found\n",
    "        target_ids = set(target_ids)\n",
    "        try:\n",
    "            with gzip.open(self.dump_path, \"rb\") as fh:\n",
    "                for entity_id, entity in ijson.kvitems(fh, \"entities\"):\n",
    "                    if entity_id in target_ids:\n",
    "                        entity[\"id\"] = entity_id\n",
    "                        found[entity_id] = entity\n",
    "                        if len(found) == len(target_ids):\n",
    "                            break\n",
    "        except Exception as exc:\n",
    "            print(f\"    [!] Context Builder stream error: {exc}\")\n",
    "        return found\n",
    "\n",
    "    def _collect_neighbor_targets(self, entities):\n",
    "        neighbors = set()\n",
    "        for entity in entities:\n",
    "            edges = 0\n",
    "            claims = entity.get(\"claims\", {})\n",
    "            for pid, statements in claims.items():\n",
    "                for claim in statements:\n",
    "                    if edges >= MAX_NEIGHBOR_EDGES:\n",
    "                        break\n",
    "                    datavalue = claim.get(\"mainsnak\", {}).get(\"datavalue\")\n",
    "                    if not datavalue:\n",
    "                        continue\n",
    "                    value = datavalue.get(\"value\")\n",
    "                    if isinstance(value, dict) and value.get(\"entity-type\") in {\"item\", \"property\"}:\n",
    "                        target_id = value.get(\"id\")\n",
    "                        if target_id:\n",
    "                            neighbors.add(target_id)\n",
    "                            edges += 1\n",
    "                if edges >= MAX_NEIGHBOR_EDGES:\n",
    "                    break\n",
    "        return neighbors\n",
    "\n",
    "    def _fetch_labels_via_api(self, ids):\n",
    "        resolved = {}\n",
    "        for batch in chunked(list(ids), 50):\n",
    "            params = {\n",
    "                \"action\": \"wbgetentities\",\n",
    "                \"ids\": \"|\".join(batch),\n",
    "                \"props\": \"labels|descriptions\",\n",
    "            }\n",
    "            data = get_json(params)\n",
    "            if not data or \"entities\" not in data:\n",
    "                continue\n",
    "            for entity_id, entity in data[\"entities\"].items():\n",
    "                if not entity or \"missing\" in entity:\n",
    "                    continue\n",
    "                resolved[entity_id] = {\n",
    "                    \"id\": entity_id,\n",
    "                    \"labels\": entity.get(\"labels\", {}),\n",
    "                    \"descriptions\": entity.get(\"descriptions\", {}),\n",
    "                }\n",
    "        return resolved\n",
    "\n",
    "    def _assemble_world_state(self, focus_entity, neighbor_entities, property_id, property_entity):\n",
    "        focus_node = {\n",
    "            \"qid\": focus_entity.get(\"id\"),\n",
    "            \"label\": pick_label(focus_entity),\n",
    "            \"description\": pick_description(focus_entity),\n",
    "            \"properties\": self._extract_properties(focus_entity),\n",
    "        }\n",
    "        neighborhood_snapshot = self._build_neighborhood_snapshot(focus_entity, neighbor_entities)\n",
    "        constraint_metadata = self._extract_constraints(property_id, property_entity)\n",
    "        return {\n",
    "            \"focus_node\": focus_node,\n",
    "            \"neighborhood_snapshot\": neighborhood_snapshot,\n",
    "            \"constraint_metadata\": constraint_metadata,\n",
    "        }\n",
    "\n",
    "    def _extract_properties(self, entity):\n",
    "        properties = {}\n",
    "        claims = entity.get(\"claims\", {})\n",
    "        for pid, statements in claims.items():\n",
    "            values = []\n",
    "            for claim in statements:\n",
    "                snak = claim.get(\"mainsnak\", {})\n",
    "                if snak.get(\"snaktype\") != \"value\":\n",
    "                    continue\n",
    "                datavalue = snak.get(\"datavalue\")\n",
    "                if not datavalue:\n",
    "                    continue\n",
    "                values.append(format_datavalue(datavalue.get(\"value\")))\n",
    "                if len(values) >= MAX_PROPERTY_VALUES:\n",
    "                    break\n",
    "            if values:\n",
    "                properties[pid] = values\n",
    "        return properties\n",
    "\n",
    "    def _build_neighborhood_snapshot(self, entity, neighbor_entities):\n",
    "        edges = []\n",
    "        edge_count = 0\n",
    "        claims = entity.get(\"claims\", {})\n",
    "        for pid, statements in claims.items():\n",
    "            for claim in statements:\n",
    "                if edge_count >= MAX_NEIGHBOR_EDGES:\n",
    "                    break\n",
    "                snak = claim.get(\"mainsnak\", {})\n",
    "                datavalue = snak.get(\"datavalue\")\n",
    "                if not datavalue:\n",
    "                    continue\n",
    "                value = datavalue.get(\"value\")\n",
    "                if isinstance(value, dict) and value.get(\"entity-type\") in {\"item\", \"property\"}:\n",
    "                    target_id = value.get(\"id\")\n",
    "                    neighbor = neighbor_entities.get(target_id)\n",
    "                    edges.append(\n",
    "                        {\n",
    "                            \"property_id\": pid,\n",
    "                            \"target_qid\": target_id,\n",
    "                            \"target_label\": pick_label(neighbor),\n",
    "                            \"target_description\": pick_description(neighbor),\n",
    "                        }\n",
    "                    )\n",
    "                    edge_count += 1\n",
    "            if edge_count >= MAX_NEIGHBOR_EDGES:\n",
    "                break\n",
    "        return {\"outgoing_edges\": edges}\n",
    "\n",
    "    def _extract_constraints(self, property_id, property_entity):\n",
    "        if not property_entity:\n",
    "            return {\"property_id\": property_id, \"constraints\": []}\n",
    "        constraints = []\n",
    "        constraint_claims = property_entity.get(\"claims\", {}).get(\"P2302\", [])\n",
    "        for claim in constraint_claims:\n",
    "            snak = claim.get(\"mainsnak\", {})\n",
    "            datavalue = snak.get(\"datavalue\")\n",
    "            constraint_qid = None\n",
    "            if datavalue:\n",
    "                value = datavalue.get(\"value\")\n",
    "                if isinstance(value, dict):\n",
    "                    constraint_qid = value.get(\"id\")\n",
    "            qualifiers = claim.get(\"qualifiers\", {})\n",
    "            qualifier_parts = []\n",
    "            for qualifier_pid, qualifier_values in qualifiers.items():\n",
    "                rendered = []\n",
    "                for qualifier in qualifier_values:\n",
    "                    dv = qualifier.get(\"datavalue\")\n",
    "                    if dv:\n",
    "                        rendered.append(format_datavalue(dv.get(\"value\")))\n",
    "                if rendered:\n",
    "                    qualifier_parts.append(f\"{qualifier_pid}: {', '.join(rendered)}\")\n",
    "            summary = \"; \".join(qualifier_parts) if qualifier_parts else \"No qualifiers recorded.\"\n",
    "            constraints.append(\n",
    "                {\n",
    "                    \"constraint_type\": constraint_qid,\n",
    "                    \"rule_summary\": summary,\n",
    "                }\n",
    "            )\n",
    "        return {\n",
    "            \"property_id\": property_id,\n",
    "            \"constraints\": constraints,\n",
    "        }\n",
    "\n",
    "def parse_iso8601(raw_ts):\n",
    "    if not raw_ts:\n",
    "        return None\n",
    "    normalized = raw_ts[:-1] + \"+00:00\" if raw_ts.endswith(\"Z\") else raw_ts\n",
    "    try:\n",
    "        dt = datetime.fromisoformat(normalized)\n",
    "    except ValueError:\n",
    "        return None\n",
    "    if dt.tzinfo is None:\n",
    "        dt = dt.replace(tzinfo=timezone.utc)\n",
    "    return dt\n",
    "\n",
    "\n",
    "def format_timestamp(dt):\n",
    "    return dt.astimezone(timezone.utc).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "\n",
    "def compute_revision_window(report_date):\n",
    "    end_dt = parse_iso8601(report_date)\n",
    "    if not end_dt:\n",
    "        return None, None\n",
    "    start_dt = end_dt - timedelta(days=REVISION_LOOKBACK_DAYS)\n",
    "    return format_timestamp(start_dt), format_timestamp(end_dt)\n",
    "\n",
    "\n",
    "def format_datavalue(value):\n",
    "    if isinstance(value, dict):\n",
    "        if \"time\" in value:\n",
    "            return value[\"time\"]\n",
    "        if \"id\" in value:\n",
    "            return value[\"id\"]\n",
    "        if \"text\" in value:\n",
    "            lang = value.get(\"language\")\n",
    "            return f\"{value['text']}@{lang}\" if lang else value[\"text\"]\n",
    "        if \"amount\" in value:\n",
    "            unit = value.get(\"unit\", \"\")\n",
    "            return f\"{value['amount']} {unit}\".strip()\n",
    "        if \"latitude\" in value and \"longitude\" in value:\n",
    "            return f\"{value['latitude']},{value['longitude']}\"\n",
    "    return str(value)\n",
    "\n",
    "\n",
    "def summarize_claims(claims):\n",
    "    if not claims:\n",
    "        return (\"MISSING\",), [\"MISSING\"]\n",
    "    signature_parts = []\n",
    "    display_values = []\n",
    "    for claim in claims:\n",
    "        snak = claim.get(\"mainsnak\", {})\n",
    "        snak_type = snak.get(\"snaktype\", \"\").upper() or \"UNKNOWN\"\n",
    "        if snak_type == \"VALUE\":\n",
    "            value_str = format_datavalue(snak.get(\"datavalue\", {}).get(\"value\"))\n",
    "        else:\n",
    "            value_str = snak_type\n",
    "        signature_parts.append(f\"{snak_type}:{value_str}\")\n",
    "        display_values.append(value_str)\n",
    "    if not signature_parts:\n",
    "        return (\"MISSING\",), [\"MISSING\"]\n",
    "    return tuple(sorted(signature_parts)), display_values\n",
    "\n",
    "\n",
    "def classify_action(previous_signature, current_signature):\n",
    "    if current_signature == (\"MISSING\",):\n",
    "        return \"DELETE\"\n",
    "    if previous_signature == (\"MISSING\",):\n",
    "        return \"CREATE\"\n",
    "    return \"UPDATE\"\n",
    "\n",
    "\n",
    "def get_json(params=None, *, endpoint=API_ENDPOINT, with_format=True):\n",
    "    query = dict(params or {})\n",
    "    if with_format:\n",
    "        query.setdefault(\"format\", \"json\")\n",
    "        query.setdefault(\"formatversion\", 2)\n",
    "    for attempt in range(4):\n",
    "        try:\n",
    "            response = requests.get(\n",
    "                endpoint,\n",
    "                headers=HEADERS,\n",
    "                params=query if query else None,\n",
    "                timeout=API_TIMEOUT,\n",
    "            )\n",
    "            if response.status_code == 200:\n",
    "                return response.json()\n",
    "            if response.status_code == 429:\n",
    "                sleep_for = 2**attempt\n",
    "                print(f\"    [!] Rate limited. Sleeping {sleep_for}s...\")\n",
    "                time.sleep(sleep_for)\n",
    "            else:\n",
    "                print(f\"    [!] HTTP {response.status_code} for {endpoint}\")\n",
    "        except Exception as exc:\n",
    "            print(f\"    [!] Exception: {exc}\")\n",
    "        time.sleep(1)\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_current_state(qid, property_id):\n",
    "    params = {\n",
    "        \"action\": \"wbgetentities\",\n",
    "        \"ids\": qid,\n",
    "        \"props\": \"claims\",\n",
    "    }\n",
    "    data = get_json(params)\n",
    "    if not data or \"entities\" not in data:\n",
    "        return None\n",
    "    entity = data[\"entities\"].get(qid)\n",
    "    if not entity or \"missing\" in entity:\n",
    "        return None\n",
    "    claims = entity.get(\"claims\", {}).get(property_id, [])\n",
    "    signature, values = summarize_claims(claims)\n",
    "    if signature == (\"MISSING\",):\n",
    "        return None\n",
    "    return values\n",
    "\n",
    "\n",
    "def fetch_revision_history(qid, start_time, end_time):\n",
    "    start_dt = parse_iso8601(start_time) if start_time else None\n",
    "    end_dt = parse_iso8601(end_time) if end_time else None\n",
    "    revisions = []\n",
    "    carry_revision = None\n",
    "    endpoint = REST_HISTORY_URL.format(qid=qid)\n",
    "    next_endpoint = endpoint\n",
    "    params = {\"limit\": 200}\n",
    "    batches = 0\n",
    "    truncated_by_window = False\n",
    "    reached_page_limit = False\n",
    "    api_calls = 0\n",
    "\n",
    "    while next_endpoint and batches < MAX_HISTORY_PAGES:\n",
    "        data = get_json(\n",
    "            params=params if next_endpoint == endpoint else None,\n",
    "            endpoint=next_endpoint,\n",
    "            with_format=False,\n",
    "        )\n",
    "        if not data or \"revisions\" not in data:\n",
    "            break\n",
    "        api_calls += 1\n",
    "        for rev in data.get(\"revisions\", []):\n",
    "            rev_ts = rev.get(\"timestamp\")\n",
    "            rev_dt = parse_iso8601(rev_ts)\n",
    "            if not rev_dt:\n",
    "                continue\n",
    "            if end_dt and rev_dt > end_dt:\n",
    "                continue\n",
    "            if start_dt and rev_dt < start_dt:\n",
    "                truncated_by_window = True\n",
    "                if not carry_revision:\n",
    "                    carry_revision = rev\n",
    "                next_endpoint = None\n",
    "                break\n",
    "            revisions.append(rev)\n",
    "        if next_endpoint is None:\n",
    "            break\n",
    "        older_url = data.get(\"older\")\n",
    "        if not older_url:\n",
    "            break\n",
    "        next_endpoint = older_url\n",
    "        batches += 1\n",
    "        params = None\n",
    "        if start_dt and revisions:\n",
    "            oldest_dt = parse_iso8601(revisions[-1][\"timestamp\"])\n",
    "            if oldest_dt and oldest_dt < start_dt:\n",
    "                truncated_by_window = True\n",
    "                break\n",
    "    if carry_revision:\n",
    "        revisions.append(carry_revision)\n",
    "    revisions.sort(key=lambda rev: rev[\"timestamp\"])\n",
    "    if next_endpoint and batches >= MAX_HISTORY_PAGES:\n",
    "        reached_page_limit = True\n",
    "\n",
    "    history_meta = {\n",
    "        \"qid\": qid,\n",
    "        \"start_time\": start_time,\n",
    "        \"end_time\": end_time,\n",
    "        \"lookback_days\": REVISION_LOOKBACK_DAYS,\n",
    "        \"max_history_pages\": MAX_HISTORY_PAGES,\n",
    "        \"api_calls\": api_calls,\n",
    "        \"batches_used\": batches,\n",
    "        \"revisions_scanned\": len(revisions),\n",
    "        \"earliest_revision\": revisions[0][\"timestamp\"] if revisions else None,\n",
    "        \"latest_revision\": revisions[-1][\"timestamp\"] if revisions else None,\n",
    "        \"truncated_by_window\": truncated_by_window,\n",
    "        \"reached_page_limit\": reached_page_limit,\n",
    "        \"carry_revision_used\": carry_revision is not None,\n",
    "    }\n",
    "\n",
    "    return revisions, history_meta\n",
    "\n",
    "\n",
    "def get_claims_for_revision(qid, property_id, revision_id):\n",
    "    endpoint = ENTITY_DATA_URL.format(qid=qid)\n",
    "    data = get_json(\n",
    "        params={\"revision\": revision_id},\n",
    "        endpoint=endpoint,\n",
    "        with_format=False,\n",
    "    )\n",
    "    if not data or \"entities\" not in data:\n",
    "        return []\n",
    "    entity = data[\"entities\"].get(qid)\n",
    "    if not entity or \"missing\" in entity:\n",
    "        return []\n",
    "    return entity.get(\"claims\", {}).get(property_id, [])\n",
    "\n",
    "\n",
    "def extract_user(revision):\n",
    "    user = revision.get(\"user\")\n",
    "    if isinstance(user, dict):\n",
    "        return user.get(\"name\", \"unknown\")\n",
    "    return user or \"unknown\"\n",
    "\n",
    "\n",
    "def find_repair_revision(qid, property_id, start_time, end_time):\n",
    "    revisions, history_meta = fetch_revision_history(qid, start_time, end_time)\n",
    "    if not revisions:\n",
    "        return None, history_meta\n",
    "\n",
    "    previous_signature = None\n",
    "    previous_snapshot = None\n",
    "\n",
    "    for rev in revisions:\n",
    "        revision_id = rev.get(\"id\") or rev.get(\"revid\")\n",
    "        if not revision_id:\n",
    "            continue\n",
    "        current_claims = get_claims_for_revision(qid, property_id, revision_id)\n",
    "        current_signature, current_snapshot = summarize_claims(current_claims)\n",
    "        if previous_signature is not None and current_signature != previous_signature:\n",
    "            return {\n",
    "                \"repair_revision_id\": revision_id,\n",
    "                \"timestamp\": rev.get(\"timestamp\"),\n",
    "                \"action\": classify_action(previous_signature, current_signature),\n",
    "                \"old_value\": previous_snapshot,\n",
    "                \"new_value\": current_snapshot,\n",
    "                \"author\": extract_user(rev),\n",
    "            }, history_meta\n",
    "        previous_signature = current_signature\n",
    "        previous_snapshot = current_snapshot\n",
    "\n",
    "    return None, history_meta\n",
    "\n",
    "\n",
    "def process_pipeline(max_candidates=None):\n",
    "    input_file = \"repair_candidates.json\"\n",
    "    if not os.path.exists(input_file):\n",
    "        print(f\"[!] {input_file} not found.\")\n",
    "        return\n",
    "\n",
    "    with open(input_file, \"r\") as f:\n",
    "        candidates = json.load(f)\n",
    "\n",
    "    stats_logger = StatsLogger(STATS_FILE)\n",
    "    summary = {\n",
    "        \"run_id\": stats_logger.run_id,\n",
    "        \"lookback_days\": REVISION_LOOKBACK_DAYS,\n",
    "        \"max_history_pages\": MAX_HISTORY_PAGES,\n",
    "        \"total_candidates\": len(candidates),\n",
    "        \"processed\": 0,\n",
    "        \"persistence_failed\": 0,\n",
    "        \"bad_fix_date\": 0,\n",
    "        \"repairs_found\": 0,\n",
    "        \"no_diff\": 0,\n",
    "        \"no_history\": 0,\n",
    "    }\n",
    "\n",
    "    print(f\"[*] Loaded {len(candidates)} candidates. Using REST history.\")\n",
    "\n",
    "    dataset = []\n",
    "    for i, item in enumerate(candidates):\n",
    "        if max_candidates is not None and i >= max_candidates:\n",
    "            break\n",
    "        qid = item[\"qid\"]\n",
    "        pid = item[\"property_id\"]\n",
    "\n",
    "        if not qid.startswith(\"Q\"):\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n[{i + 1}/{len(candidates)}] Analyzing {qid} ({pid})...\")\n",
    "        summary[\"processed\"] += 1\n",
    "\n",
    "        record_base = {\n",
    "            \"qid\": qid,\n",
    "            \"property\": pid,\n",
    "        }\n",
    "        curr_val = None\n",
    "        if STRICT_PERSISTENCE:\n",
    "            curr_val = get_current_state(qid, pid)\n",
    "            if not curr_val:\n",
    "                print(\"    [x] Dropped: Persistence check failed (Entity/Prop missing).\")\n",
    "                summary[\"persistence_failed\"] += 1\n",
    "                stats_logger.log(\n",
    "                    {\n",
    "                        **record_base,\n",
    "                        \"result\": \"persistence_failed\",\n",
    "                        \"reason\": \"missing_current_value\",\n",
    "                    }\n",
    "                )\n",
    "                continue\n",
    "\n",
    "        report_date = item[\"fix_date\"]\n",
    "        start_time, end_time = compute_revision_window(report_date)\n",
    "        if not end_time:\n",
    "            print(\"    [x] Dropped: Could not parse fix_date.\")\n",
    "            summary[\"bad_fix_date\"] += 1\n",
    "            stats_logger.log(\n",
    "                {\n",
    "                    **record_base,\n",
    "                    \"result\": \"bad_fix_date\",\n",
    "                    \"report_date\": report_date,\n",
    "                }\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        fix_event, history_meta = find_repair_revision(\n",
    "            qid,\n",
    "            pid,\n",
    "            start_time=start_time,\n",
    "            end_time=end_time,\n",
    "        )\n",
    "\n",
    "        if fix_event:\n",
    "            print(f\"    [+] FOUND REPAIR! {fix_event['old_value']} -> {fix_event['new_value']}\")\n",
    "            summary[\"repairs_found\"] += 1\n",
    "            entry = {\n",
    "                \"id\": f\"repair_{qid}_{fix_event['repair_revision_id']}\",\n",
    "                \"qid\": qid,\n",
    "                \"property\": pid,\n",
    "                \"type\": \"TBD\",\n",
    "                \"violation_context\": {\n",
    "                    \"value\": fix_event[\"old_value\"],\n",
    "                },\n",
    "                \"repair_target\": {\n",
    "                    \"action\": fix_event[\"action\"],\n",
    "                    \"value\": fix_event[\"new_value\"],\n",
    "                    \"revision_id\": fix_event[\"repair_revision_id\"],\n",
    "                },\n",
    "                \"persistence_check\": {\n",
    "                    \"status\": \"passed\",\n",
    "                    \"current_value_2025\": curr_val,\n",
    "                },\n",
    "            }\n",
    "            dataset.append(entry)\n",
    "            stats_logger.log(\n",
    "                {\n",
    "                    **record_base,\n",
    "                    \"result\": \"repair_found\",\n",
    "                    \"history\": history_meta,\n",
    "                    \"repair_revision_id\": fix_event[\"repair_revision_id\"],\n",
    "                    \"action\": fix_event[\"action\"],\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            print(\"    [-] No clean diff found.\")\n",
    "            if history_meta:\n",
    "                summary[\"no_diff\"] += 1\n",
    "            else:\n",
    "                summary[\"no_history\"] += 1\n",
    "            stats_logger.log(\n",
    "                {\n",
    "                    **record_base,\n",
    "                    \"result\": \"no_diff\" if history_meta else \"no_history\",\n",
    "                    \"history\": history_meta,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            with open(\"wikidata_repair_eval_raw.json\", \"w\") as out:\n",
    "                json.dump(dataset, out, indent=2)\n",
    "\n",
    "    world_states = {}\n",
    "    if dataset:\n",
    "        builder = WorldStateBuilder(LATEST_DUMP_PATH)\n",
    "        world_states = builder.build(dataset)\n",
    "        if world_states:\n",
    "            for entry in dataset:\n",
    "                context = world_states.get(entry[\"id\"])\n",
    "                if context:\n",
    "                    entry[\"world_state\"] = context\n",
    "            with open(WORLD_STATE_FILE, \"w\", encoding=\"utf-8\") as world_file:\n",
    "                json.dump(world_states, world_file, indent=2)\n",
    "\n",
    "    with open(\"wikidata_repair_eval_raw.json\", \"w\") as out:\n",
    "        json.dump(dataset, out, indent=2)\n",
    "\n",
    "    with open(SUMMARY_FILE, \"w\", encoding=\"utf-8\") as summary_file:\n",
    "        json.dump(summary, summary_file, indent=2)\n",
    "\n",
    "    print(f\"\\n[+] Extraction Complete. Saved {len(dataset)} verified repairs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "63b462a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Loaded 304 candidates. Using Action API.\n",
      "\n",
      "[1/304] Analyzing Q223311 (P569)...\n",
      "    [-] No clean diff found.\n",
      "\n",
      "[2/304] Analyzing Q134708713 (P569)...\n",
      "    [x] Dropped: Persistence check failed (Entity/Prop missing).\n",
      "\n",
      "[3/304] Analyzing Q1930855 (P569)...\n",
      "    [x] Dropped: Persistence check failed (Entity/Prop missing).\n",
      "\n",
      "[4/304] Analyzing Q8975832 (P569)...\n",
      "    [-] No clean diff found.\n",
      "\n",
      "[5/304] Analyzing Q18356450 (P569)...\n",
      "    [x] Dropped: Persistence check failed (Entity/Prop missing).\n",
      "\n",
      "[6/304] Analyzing Q137286504 (P569)...\n",
      "    [-] No clean diff found.\n",
      "\n",
      "[7/304] Analyzing Q57584664 (P569)...\n",
      "    [-] No clean diff found.\n",
      "\n",
      "[8/304] Analyzing Q4110950 (P569)...\n",
      "    [-] No clean diff found.\n",
      "\n",
      "[9/304] Analyzing Q9531806 (P569)...\n",
      "    [x] Dropped: Persistence check failed (Entity/Prop missing).\n",
      "\n",
      "[10/304] Analyzing Q137264893 (P569)...\n",
      "    [-] No clean diff found.\n",
      "\n",
      "[11/304] Analyzing Q744961 (P569)...\n",
      "    [x] Dropped: Persistence check failed (Entity/Prop missing).\n",
      "\n",
      "[12/304] Analyzing Q23894233 (P569)...\n",
      "    [x] Dropped: Persistence check failed (Entity/Prop missing).\n",
      "\n",
      "[13/304] Analyzing Q135437429 (P569)...\n",
      "    [x] Dropped: Persistence check failed (Entity/Prop missing).\n",
      "\n",
      "[14/304] Analyzing Q2081008 (P569)...\n",
      "    [-] No clean diff found.\n",
      "\n",
      "[15/304] Analyzing Q119834 (P569)...\n",
      "    [-] No clean diff found.\n",
      "\n",
      "[16/304] Analyzing Q137260663 (P569)...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mprocess_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 237\u001b[39m, in \u001b[36mprocess_pipeline\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    234\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m    [x] Dropped: Could not parse fix_date.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    235\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m237\u001b[39m fix_event = \u001b[43mfind_repair_revision\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m                                 \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    239\u001b[39m \u001b[43m                                 \u001b[49m\u001b[43mend_time\u001b[49m\u001b[43m=\u001b[49m\u001b[43mend_time\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    241\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fix_event:\n\u001b[32m    242\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m    [+] FOUND REPAIR! \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfix_event[\u001b[33m'\u001b[39m\u001b[33mold_value\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m -> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfix_event[\u001b[33m'\u001b[39m\u001b[33mnew_value\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 175\u001b[39m, in \u001b[36mfind_repair_revision\u001b[39m\u001b[34m(qid, property_id, start_time, end_time)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m rev \u001b[38;5;129;01min\u001b[39;00m revisions:\n\u001b[32m    169\u001b[39m     content_params = {\n\u001b[32m    170\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33maction\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mwbgetentities\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    171\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mids\u001b[39m\u001b[33m\"\u001b[39m: qid,\n\u001b[32m    172\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mrevision\u001b[39m\u001b[33m\"\u001b[39m: rev[\u001b[33m'\u001b[39m\u001b[33mrevid\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m    173\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mprops\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mclaims\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    174\u001b[39m     }\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     rev_data = \u001b[43mget_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m rev_data \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mentities\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m rev_data:\n\u001b[32m    178\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 93\u001b[39m, in \u001b[36mget_json\u001b[39m\u001b[34m(params)\u001b[39m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m attempt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m3\u001b[39m):\n\u001b[32m     92\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m         response = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mAPI_ENDPOINT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mHEADERS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mAPI_TIMEOUT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     94\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m response.status_code == \u001b[32m200\u001b[39m:\n\u001b[32m     95\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m response.json()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Code\\kg-benchmark\\.venv\\Lib\\site-packages\\requests\\api.py:73\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(url, params, **kwargs)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget\u001b[39m(url, params=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m     63\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mget\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Code\\kg-benchmark\\.venv\\Lib\\site-packages\\requests\\api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Code\\kg-benchmark\\.venv\\Lib\\site-packages\\requests\\sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Code\\kg-benchmark\\.venv\\Lib\\site-packages\\requests\\sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Code\\kg-benchmark\\.venv\\Lib\\site-packages\\requests\\adapters.py:644\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    641\u001b[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    659\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Code\\kg-benchmark\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Code\\kg-benchmark\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:464\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    462\u001b[39m     \u001b[38;5;66;03m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[32m    463\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m464\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    465\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    466\u001b[39m         \u001b[38;5;28mself\u001b[39m._raise_timeout(err=e, url=url, timeout_value=conn.timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Code\\kg-benchmark\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1093\u001b[39m, in \u001b[36mHTTPSConnectionPool._validate_conn\u001b[39m\u001b[34m(self, conn)\u001b[39m\n\u001b[32m   1091\u001b[39m \u001b[38;5;66;03m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[32m   1092\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m conn.is_closed:\n\u001b[32m-> \u001b[39m\u001b[32m1093\u001b[39m     \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1095\u001b[39m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n\u001b[32m   1096\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn.is_verified \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn.proxy_is_verified:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Code\\kg-benchmark\\.venv\\Lib\\site-packages\\urllib3\\connection.py:796\u001b[39m, in \u001b[36mHTTPSConnection.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    793\u001b[39m     \u001b[38;5;66;03m# Remove trailing '.' from fqdn hostnames to allow certificate validation\u001b[39;00m\n\u001b[32m    794\u001b[39m     server_hostname_rm_dot = server_hostname.rstrip(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m796\u001b[39m     sock_and_verified = \u001b[43m_ssl_wrap_socket_and_match_hostname\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m        \u001b[49m\u001b[43msock\u001b[49m\u001b[43m=\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcert_reqs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcert_reqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m        \u001b[49m\u001b[43mssl_version\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mssl_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m        \u001b[49m\u001b[43mssl_minimum_version\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mssl_minimum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    801\u001b[39m \u001b[43m        \u001b[49m\u001b[43mssl_maximum_version\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mssl_maximum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[43m        \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    803\u001b[39m \u001b[43m        \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    804\u001b[39m \u001b[43m        \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    805\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcert_file\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    806\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_file\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    807\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    808\u001b[39m \u001b[43m        \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_hostname_rm_dot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    809\u001b[39m \u001b[43m        \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mssl_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    810\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    811\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43massert_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    812\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_fingerprint\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43massert_fingerprint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    813\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    814\u001b[39m     \u001b[38;5;28mself\u001b[39m.sock = sock_and_verified.socket\n\u001b[32m    816\u001b[39m \u001b[38;5;66;03m# If an error occurs during connection/handshake we may need to release\u001b[39;00m\n\u001b[32m    817\u001b[39m \u001b[38;5;66;03m# our lock so another connection can probe the origin.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Code\\kg-benchmark\\.venv\\Lib\\site-packages\\urllib3\\connection.py:975\u001b[39m, in \u001b[36m_ssl_wrap_socket_and_match_hostname\u001b[39m\u001b[34m(sock, cert_reqs, ssl_version, ssl_minimum_version, ssl_maximum_version, cert_file, key_file, key_password, ca_certs, ca_cert_dir, ca_cert_data, assert_hostname, assert_fingerprint, server_hostname, ssl_context, tls_in_tls)\u001b[39m\n\u001b[32m    972\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_ipaddress(normalized):\n\u001b[32m    973\u001b[39m         server_hostname = normalized\n\u001b[32m--> \u001b[39m\u001b[32m975\u001b[39m ssl_sock = \u001b[43mssl_wrap_socket\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    976\u001b[39m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[43m=\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    977\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeyfile\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    978\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcertfile\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    979\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    981\u001b[39m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    982\u001b[39m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    985\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    988\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    989\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m assert_fingerprint:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Code\\kg-benchmark\\.venv\\Lib\\site-packages\\urllib3\\util\\ssl_.py:461\u001b[39m, in \u001b[36mssl_wrap_socket\u001b[39m\u001b[34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[39m\n\u001b[32m    459\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ca_certs \u001b[38;5;129;01mor\u001b[39;00m ca_cert_dir \u001b[38;5;129;01mor\u001b[39;00m ca_cert_data:\n\u001b[32m    460\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m461\u001b[39m         \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_verify_locations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    462\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    463\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "process_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
